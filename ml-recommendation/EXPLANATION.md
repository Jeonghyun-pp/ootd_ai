# 토크나이저, MLP, 대조학습 설명

## 1. 토크나이저 (Tokenizer) 🔤

### 개념
**토크나이저**는 텍스트를 컴퓨터가 이해할 수 있는 숫자로 변환하는 도구입니다.

### 예시
```
입력: "미니멀 데이트 코디"

토크나이저 처리:
1. 단어 분리: ["미니멀", "데이트", "코디"]
2. 숫자 변환: [101, 234, 567] (각 단어에 고유 번호 부여)
3. 패딩/자르기: [101, 234, 567, 0, 0, 0, ...] (고정 길이로 맞춤)
```

### 왜 필요한가?
- 컴퓨터는 텍스트를 직접 이해 못함 → 숫자로 변환 필요
- "미니멀"과 "캐주얼"의 의미 차이를 숫자로 표현
- 한국어 특성(띄어쓰기, 조사 등) 처리

### 코디 추천에서의 역할
```python
# 사용자 입력
mood = "미니멀 데이트"
comment = "면접 준비"

# 토크나이저로 변환
tokens = tokenizer(mood + " " + comment)
# → [101, 234, 567, 890, ...] (숫자 배열)

# 이 숫자들을 모델에 입력
```

---

## 2. MLP (Multi-Layer Perceptron) 🧠

### 개념
**MLP**는 여러 층의 뉴런(노드)으로 구성된 인공신경망입니다.

### 구조
```
입력층 → 은닉층1 → 은닉층2 → 출력층
  ↓        ↓        ↓        ↓
[특징들] → [256개] → [128개] → [점수]
```

### 예시 (코디 추천)
```
입력:
- 텍스트 임베딩: [0.2, 0.5, -0.1, ...] (256차원)
- 기온: 5.1
- 체감온도: 4.6
- 강수량: 0.0
- 카테고리: [1, 2, 0] (상의, 하의, 아우터)

↓ MLP 통과

은닉층1 (256개 뉴런):
- 각 뉴런이 입력을 조합하여 새로운 특징 추출
- 예: "기온이 낮고 미니멀 스타일" → 뉴런1 활성화

은닉층2 (128개 뉴런):
- 더 복잡한 패턴 학습
- 예: "겨울 + 미니멀 + 데이트" → 뉴런2 활성화

출력층 (1개 뉴런):
- 최종 점수: 0.87 (87% 매칭)
```

### 왜 필요한가?
- 단순 규칙으로는 복잡한 패턴 학습 불가
- 여러 특징을 조합하여 의미 있는 패턴 발견
- 예: "겨울 + 미니멀 + 데이트" = 따뜻한 미니멀 룩

### 코디 추천에서의 역할
```python
# 입력 특징들
features = [텍스트벡터, 기온, 체감온도, 카테고리...]

# MLP 통과
hidden1 = ReLU(Linear(features))  # 첫 번째 층
hidden2 = ReLU(Linear(hidden1))   # 두 번째 층
score = Sigmoid(Linear(hidden2))  # 최종 점수 (0-1)

# 출력: 0.87 → 이 코디가 87% 적합하다!
```

---

## 3. 대조학습 (Contrastive Learning) ⚖️

### 개념
**대조학습**은 "비슷한 것끼리는 가깝게, 다른 것끼리는 멀게" 배치하는 학습 방법입니다.

### 예시 (코디 추천)

#### 일반 학습 (Supervised Learning)
```
입력: "미니멀 데이트"
옷 조합: [화이트 셔츠, 블랙 슬랙스]
라벨: 점수 0.9 (매우 적합)

→ 모델: "이 조합은 0.9점이다" 학습
```

#### 대조학습 (Contrastive Learning)
```
Anchor (기준): "미니멀 데이트" + [화이트 셔츠, 블랙 슬랙스] → 점수 0.9

Positive (비슷한 것): "심플 데이트" + [화이트 셔츠, 블랙 슬랙스] → 점수 0.85
→ 비슷한 의미이므로 점수도 비슷하게 학습

Negative (다른 것): "스트릿 파티" + [화이트 셔츠, 블랙 슬랙스] → 점수 0.3
→ 다른 의미이므로 점수도 다르게 학습

목표:
- Positive는 Anchor와 가깝게 (비슷한 점수)
- Negative는 Anchor와 멀게 (다른 점수)
```

### 시각화
```
일반 학습:
[미니멀 데이트] → 0.9점
[스트릿 파티] → 0.3점
(각각 독립적으로 학습)

대조학습:
[미니멀 데이트] ←→ [심플 데이트] (가깝게)
[미니멀 데이트] ←→ [스트릿 파티] (멀게)
(상대적 관계 학습)
```

### 왜 필요한가?
- 일반 학습: 각 케이스를 독립적으로 학습
- 대조학습: **상대적 관계**를 학습하여 더 정확함
- 예: "미니멀"과 "심플"은 비슷하지만, "스트릿"과는 다름

### 코디 추천에서의 역할
```python
# 대조학습 손실 함수
def contrastive_loss(anchor_score, positive_score, negative_score):
    # Positive는 Anchor와 가깝게
    positive_loss = (anchor_score - positive_score) ** 2
    
    # Negative는 Anchor와 멀게 (최소 거리 보장)
    negative_loss = max(0, margin - (anchor_score - negative_score)) ** 2
    
    return positive_loss + negative_loss

# 학습 결과:
# "미니멀 데이트" + 코디A = 0.9
# "심플 데이트" + 코디A = 0.85 (비슷함)
# "스트릿 파티" + 코디A = 0.3 (다름)
```

---

## 전체 흐름 정리 🔄

### 1단계: 토크나이저
```
"미니멀 데이트" 
  ↓ 토크나이저
[101, 234, 567, ...] (숫자 배열)
  ↓ 임베딩
[0.2, 0.5, -0.1, ...] (의미 벡터)
```

### 2단계: 특징 결합
```
텍스트 벡터: [0.2, 0.5, ...]
기온: 5.1
체감온도: 4.6
카테고리: [1, 2, 0]
  ↓ 결합
[0.2, 0.5, ..., 5.1, 4.6, 1, 2, 0]
```

### 3단계: MLP 통과
```
입력 → 은닉층1 (256) → 은닉층2 (128) → 출력 (1)
  ↓         ↓              ↓            ↓
특징들    패턴1          패턴2         점수
```

### 4단계: 대조학습 (학습 시)
```
Anchor: "미니멀 데이트" + 코디A → 0.9
Positive: "심플 데이트" + 코디A → 0.85 (가깝게)
Negative: "스트릿 파티" + 코디A → 0.3 (멀게)

→ 모델이 상대적 관계 학습
```

---

## 실제 코드 예시

### 토크나이저 사용
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('monologg/kobert')
text = "미니멀 데이트 코디"

# 토크나이징
encoded = tokenizer(text, return_tensors='pt')
# encoded['input_ids'] = [101, 234, 567, 890, ...]
```

### MLP 모델
```python
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(256, 128),  # 입력 → 은닉층1
            nn.ReLU(),
            nn.Linear(128, 64),   # 은닉층1 → 은닉층2
            nn.ReLU(),
            nn.Linear(64, 1),     # 은닉층2 → 출력
            nn.Sigmoid()          # 0-1 점수
        )
    
    def forward(self, x):
        return self.layers(x)
```

### 대조학습 손실
```python
def contrastive_loss(anchor, positive, negative, margin=0.5):
    # Positive는 가깝게
    pos_loss = (anchor - positive) ** 2
    
    # Negative는 멀게 (최소 margin 거리)
    neg_loss = torch.clamp(margin - (anchor - negative), min=0) ** 2
    
    return pos_loss + neg_loss
```

---

## 요약 📝

1. **토크나이저**: 텍스트 → 숫자 변환
2. **MLP**: 숫자들을 조합하여 패턴 학습 → 점수 출력
3. **대조학습**: 상대적 관계를 학습하여 더 정확한 추천

이 세 가지를 결합하면:
- 사용자 입력("미니멀 데이트")을 이해하고
- 복잡한 패턴을 학습하여
- 정확한 코디 추천을 할 수 있습니다!
